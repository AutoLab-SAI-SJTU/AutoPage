<!DOCTYPE html>
<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <link rel="stylesheet" type="text/css" media="screen" href="assets/css/style.css%3Fv=187e29baba1c86d958a2dfc2592b8e5002061d24.css">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Vision-LSTM (ViL) | xLSTM as Generic Vision Backbone</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Vision-LSTM (ViL)" />
<meta name="author" content="Benedikt Alkin" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="xLSTM as Generic Vision Backbone" />
<meta property="og:description" content="xLSTM as Generic Vision Backbone" />
<link rel="canonical" href="index.html" />
<meta property="og:url" content="https://nx-ai.github.io/vision-lstm/" />
<meta property="og:site_name" content="Vision-LSTM (ViL)" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Vision-LSTM (ViL)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","author":{"@type":"Person","name":"Benedikt Alkin"},"description":"xLSTM as Generic Vision Backbone","headline":"Vision-LSTM (ViL)","name":"Vision-LSTM (ViL)","url":"https://nx-ai.github.io/vision-lstm/"}</script>
<!-- End Jekyll SEO tag -->

    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/vision-lstm/favicon.ico" -->

<!-- end custom head snippets -->

  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          
            <a id="forkme_banner" href="https://github.com/NX-AI/vision-lstm">View on GitHub</a>
          

          <h1 id="project_title">Vision-LSTM (ViL)</h1>
          <h2 id="project_tagline">xLSTM as Generic Vision Backbone</h2>

          
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        
<p>[<a href="https://github.com/nx-ai/vision-lstm">Code</a>] 
[<a href="https://arxiv.org/abs/2406.04303">Paper</a>] 
[<a href="https://github.com/NX-AI/vision-lstm?tab=readme-ov-file#pre-trained-models">Models</a>] 
[<a href="https://youtu.be/80kc3hscTTg">Codebase Demo Video</a>]
[<a href="https://github.com/NX-AI/vision-lstm?tab=readme-ov-file#citation">BibTeX</a>]</p>

<p>We introduce Vision-LSTM (ViL), an adaption of <a href="https://arxiv.org/abs/2405.04517">xLSTM</a> to computer vision.
In order to adjust xLSTM (an autoregressive model) to better handle non-autoregressive inputs such as images,
we employ alternating bi-directional mLSTM blocks. Odd blocks process the image row-wise from top left to bottom right, while
even blocks process the image from bottom right to top left.</p>

<p align="center">
<img width="100%" alt="vision_lstm_schematic" src="https://raw.githubusercontent.com/nx-ai/vision-lstm/main/docs/imgs/schematic.svg" />
</p>

<p>We pre-train ViL models on ImageNet-1K, for which we attach a linear classification head and use the 
concatenation of the first and last token as input to the classifier. Afterwards, the pre-trained model is evaluated
also on transfer classification and semantic segmentation downstream tasks.</p>

<p>Our new model performs favorably against heavily optimized ViT baselines such as <a href="https://arxiv.org/abs/2012.12877">DeiT</a>
and <a href="https://arxiv.org/abs/2401.09417">Vision-Mamba</a> (Vim) on ImageNet-1K classification, ADE20K semantic segmentation
and VTAB-1K transfer classification.</p>

<p align="center">
<img width="100%" alt="flops_vs_performance" src="https://raw.githubusercontent.com/nx-ai/vision-lstm/main/docs/imgs/flops_vs_performance.png" />
</p>

<p>We compare against a variety of isotropic models on ImageNet-1K, where ViL performs best on the tiny and small model 
scale, outperforming transformers (DeiT), CNNs (ConvNeXt) and vision adaptions of other sequential models such as
RWKV (VRWKV) and Mamba (Vim, MambaÂ®).
On the base model scale, ViL achieves good results but heavily optimized transformer models, that underwent multiple
cycles of hyperparameter tuning, (DeiT-III) perform best.</p>

<p align="center">
<img width="80%" alt="results_imagenet" src="https://raw.githubusercontent.com/nx-ai/vision-lstm/main/docs/imgs/results_imagenet.png" />
</p>

<p>On ADE20K semantic segmentation, ViL also performs very well, even outperforming DeiT-III-B despite the lower 
ImageNet-1K accuracy.</p>

<p align="center">
<img width="80%" alt="results_ade20k" src="https://raw.githubusercontent.com/nx-ai/vision-lstm/main/docs/imgs/results_ade20k.png" />
</p>

<p>On a diverse set of 19 transfer classification tasks contained in VTAB-1K benchmark, ViL performs best on the average over all 19
datasets. Notably, ViL performs exceptionally well on the 8 structured datasets of the VTAB-1K benchmark, even 
outperforming DeiT-III-B.</p>

<p align="center">
<img width="80%" alt="results_vtab1k" src="https://raw.githubusercontent.com/nx-ai/vision-lstm/main/docs/imgs/results_vtab1k.png" />
</p>

      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        
        <p class="copyright">Vision-LSTM (ViL) maintained by <a href="https://github.com/NX-AI">NX-AI</a></p>
        
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>
  </body>
</html>
