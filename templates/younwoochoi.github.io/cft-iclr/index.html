<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="description" content="Simple project page template for your research paper, built with Astro and Tailwind CSS"><meta name="viewport" content="width=device-width"><meta name="generator" content="Astro v5.1.6"><link rel="icon" type="image/svg+xml" href="favicon.svg"><meta property="og:title" content="Teaching LLMs How to Learn with Contextual Fine-Tuning"><meta property="og:description" content="Simple project page template for your research paper, built with Astro and Tailwind CSS"><meta property="og:type" content="website"><meta property="og:image" content="/cft-iclr/screenshot.png"><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.css" integrity="sha384-WsHMgfkABRyG494OmuiNmkAOk8nhO1qE+Y6wns6v+EoNoTNxrWxYpl5ZYWFOLPCM" crossorigin="anonymous"><title>Teaching LLMs How to Learn with Contextual Fine-Tuning</title><link rel="stylesheet" href="_astro/index.B-MZGRE8.css"></head> <body class="flex flex-col gap-4 items-center pt-12 pb-6 w-full *:px-6 *:max-w-[60rem]"> <header class="flex flex-col gap-10 items-center mb-6"> <h1>Teaching LLMs How to Learn with Contextual Fine-Tuning</h1> <div class="flex flex-col gap-6 items-center"> <div class="flex flex-row gap-x-8 gap-y-4 flex-wrap justify-center"> <div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row"> Younwoo Choi <sup class="text-xl"> * </sup> </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row"> Muhammad Adil Asif <sup class="text-xl"> * </sup> </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row"> Ziwen Han <sup class="text-xl"> † </sup> </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row"> John Willes  </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row"> Rahul G. Krishnan  </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row">   </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row"> University of Toronto  </span>  </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row"> Vector Institute  </span>  </div> </div> <p class="text-center">ICLR 2025</p> <p class="text-sm text-center">  <sup>*</sup>Equal contribution, <sup>†</sup>Work conducted while at the Vector Institute </p> <div class="flex flex-row flex-wrap justify-center gap-2"> <a href="https://arxiv.org/abs/2503.09032" class="flex flex-row bg-zinc-800 text-white rounded-full gap-2 items-center text-lg px-5 py-2 hover:bg-zinc-900 hover:no-underline"> <svg width="1em" height="1em" data-icon="ri:file-pdf-2-line">   <symbol id="ai:ri:file-pdf-2-line" viewBox="0 0 24 24"><path fill="currentColor" d="M5 4h10v4h4v12H5zM3.999 2A.995.995 0 0 0 3 2.992v18.016a1 1 0 0 0 .993.992h16.014A1 1 0 0 0 21 20.992V7l-5-5zm6.5 5.5c0 1.577-.455 3.437-1.224 5.153c-.772 1.723-1.814 3.197-2.9 4.066l1.18 1.613c2.927-1.952 6.168-3.29 9.304-2.842l.457-1.939C14.644 12.661 12.5 9.99 12.5 7.5zm.6 5.972c.268-.597.505-1.216.705-1.843a9.7 9.7 0 0 0 1.706 1.966c-.982.176-1.944.465-2.875.833q.248-.471.465-.956"/></symbol><use href="#ai:ri:file-pdf-2-line"></use>  </svg> <span>Paper</span> </a><a href="https://github.com/rgklab/Contextual_Fine_Tuning" class="flex flex-row bg-zinc-800 text-white rounded-full gap-2 items-center text-lg px-5 py-2 hover:bg-zinc-900 hover:no-underline"> <svg width="1em" height="1em" data-icon="ri:github-line">   <symbol id="ai:ri:github-line" viewBox="0 0 24 24"><path fill="currentColor" d="M5.884 18.653c-.3-.2-.558-.455-.86-.816a51 51 0 0 1-.466-.579c-.463-.575-.755-.841-1.056-.95a1 1 0 1 1 .675-1.882c.752.27 1.261.735 1.947 1.588c-.094-.117.34.427.433.539c.19.227.33.365.44.438c.204.137.588.196 1.15.14c.024-.382.094-.753.202-1.095c-2.968-.726-4.648-2.64-4.648-6.396c0-1.24.37-2.356 1.058-3.292c-.218-.894-.185-1.975.302-3.192a1 1 0 0 1 .63-.582c.081-.024.127-.035.208-.047c.803-.124 1.937.17 3.415 1.096a11.7 11.7 0 0 1 2.687-.308c.912 0 1.819.104 2.684.308c1.477-.933 2.614-1.227 3.422-1.096q.128.02.218.05a1 1 0 0 1 .616.58c.487 1.216.52 2.296.302 3.19c.691.936 1.058 2.045 1.058 3.293c0 3.757-1.674 5.665-4.642 6.392c.125.415.19.878.19 1.38c0 .665-.002 1.299-.007 2.01c0 .19-.002.394-.005.706a1 1 0 0 1-.018 1.958c-1.14.227-1.984-.532-1.984-1.525l.002-.447l.005-.705c.005-.707.008-1.337.008-1.997c0-.697-.184-1.152-.426-1.361c-.661-.57-.326-1.654.541-1.751c2.966-.333 4.336-1.482 4.336-4.66c0-.955-.312-1.744-.913-2.404A1 1 0 0 1 17.2 6.19c.166-.414.236-.957.095-1.614l-.01.003c-.491.139-1.11.44-1.858.949a1 1 0 0 1-.833.135a9.6 9.6 0 0 0-2.592-.349c-.89 0-1.772.118-2.592.35a1 1 0 0 1-.829-.134c-.753-.507-1.374-.807-1.87-.947c-.143.653-.072 1.194.093 1.607a1 1 0 0 1-.189 1.045c-.597.655-.913 1.458-.913 2.404c0 3.172 1.371 4.328 4.322 4.66c.865.097 1.202 1.177.545 1.748c-.193.168-.43.732-.43 1.364v3.15c0 .985-.834 1.725-1.96 1.528a1 1 0 0 1-.04-1.962v-.99c-.91.061-1.661-.088-2.254-.485"/></symbol><use href="#ai:ri:github-line"></use>  </svg> <span>Code</span> </a><a href="https://huggingface.co/datasets/ywchoi/OpenMedText" class="flex flex-row bg-zinc-800 text-white rounded-full gap-2 items-center text-lg px-5 py-2 hover:bg-zinc-900 hover:no-underline"> <svg width="1em" height="1em" data-icon="simple-icons:huggingface">   <symbol id="ai:simple-icons:huggingface" viewBox="0 0 24 24"><path fill="currentColor" d="M12.025 1.13c-5.77 0-10.449 4.647-10.449 10.378c0 1.112.178 2.181.503 3.185c.064-.222.203-.444.416-.577a.96.96 0 0 1 .524-.15c.293 0 .584.124.84.284c.278.173.48.408.71.694c.226.282.458.611.684.951v-.014c.017-.324.106-.622.264-.874s.403-.487.762-.543c.3-.047.596.06.787.203s.31.313.4.467c.15.257.212.468.233.542c.01.026.653 1.552 1.657 2.54c.616.605 1.01 1.223 1.082 1.912c.055.537-.096 1.059-.38 1.572c.637.121 1.294.187 1.967.187c.657 0 1.298-.063 1.921-.178c-.287-.517-.44-1.041-.384-1.581c.07-.69.465-1.307 1.081-1.913c1.004-.987 1.647-2.513 1.657-2.539c.021-.074.083-.285.233-.542c.09-.154.208-.323.4-.467a1.08 1.08 0 0 1 .787-.203c.359.056.604.29.762.543s.247.55.265.874v.015c.225-.34.457-.67.683-.952c.23-.286.432-.52.71-.694c.257-.16.547-.284.84-.285a.97.97 0 0 1 .524.151c.228.143.373.388.43.625l.006.04a10.3 10.3 0 0 0 .534-3.273c0-5.731-4.678-10.378-10.449-10.378M8.327 6.583a1.5 1.5 0 0 1 .713.174a1.487 1.487 0 0 1 .617 2.013c-.183.343-.762-.214-1.102-.094c-.38.134-.532.914-.917.71a1.487 1.487 0 0 1 .69-2.803m7.486 0a1.487 1.487 0 0 1 .689 2.803c-.385.204-.536-.576-.916-.71c-.34-.12-.92.437-1.103.094a1.487 1.487 0 0 1 .617-2.013a1.5 1.5 0 0 1 .713-.174m-10.68 1.55a.96.96 0 1 1 0 1.921a.96.96 0 0 1 0-1.92m13.838 0a.96.96 0 1 1 0 1.92a.96.96 0 0 1 0-1.92M8.489 11.458c.588.01 1.965 1.157 3.572 1.164c1.607-.007 2.984-1.155 3.572-1.164c.196-.003.305.12.305.454c0 .886-.424 2.328-1.563 3.202c-.22-.756-1.396-1.366-1.63-1.32q-.011.001-.02.006l-.044.026l-.01.008l-.03.024q-.018.017-.035.036l-.032.04a1 1 0 0 0-.058.09l-.014.025q-.049.088-.11.19a1 1 0 0 1-.083.116a1.2 1.2 0 0 1-.173.18q-.035.029-.075.058a1.3 1.3 0 0 1-.251-.243a1 1 0 0 1-.076-.107c-.124-.193-.177-.363-.337-.444c-.034-.016-.104-.008-.2.022q-.094.03-.216.087q-.06.028-.125.063l-.13.074q-.067.04-.136.086a3 3 0 0 0-.135.096a3 3 0 0 0-.26.219a2 2 0 0 0-.12.121a2 2 0 0 0-.106.128l-.002.002a2 2 0 0 0-.09.132l-.001.001a1.2 1.2 0 0 0-.105.212q-.013.036-.024.073c-1.139-.875-1.563-2.317-1.563-3.203c0-.334.109-.457.305-.454m.836 10.354c.824-1.19.766-2.082-.365-3.194c-1.13-1.112-1.789-2.738-1.789-2.738s-.246-.945-.806-.858s-.97 1.499.202 2.362c1.173.864-.233 1.45-.685.64c-.45-.812-1.683-2.896-2.322-3.295s-1.089-.175-.938.647s2.822 2.813 2.562 3.244s-1.176-.506-1.176-.506s-2.866-2.567-3.49-1.898s.473 1.23 2.037 2.16c1.564.932 1.686 1.178 1.464 1.53s-3.675-2.511-4-1.297c-.323 1.214 3.524 1.567 3.287 2.405c-.238.839-2.71-1.587-3.216-.642c-.506.946 3.49 2.056 3.522 2.064c1.29.33 4.568 1.028 5.713-.624m5.349 0c-.824-1.19-.766-2.082.365-3.194c1.13-1.112 1.789-2.738 1.789-2.738s.246-.945.806-.858s.97 1.499-.202 2.362c-1.173.864.233 1.45.685.64c.451-.812 1.683-2.896 2.322-3.295s1.089-.175.938.647s-2.822 2.813-2.562 3.244s1.176-.506 1.176-.506s2.866-2.567 3.49-1.898s-.473 1.23-2.037 2.16c-1.564.932-1.686 1.178-1.464 1.53s3.675-2.511 4-1.297c.323 1.214-3.524 1.567-3.287 2.405c.238.839 2.71-1.587 3.216-.642c.506.946-3.49 2.056-3.522 2.064c-1.29.33-4.568 1.028-5.713-.624"/></symbol><use href="#ai:simple-icons:huggingface"></use>  </svg> <span>Dataset</span> </a> </div> </div> </header>
<img src="_astro/mainfig.B0B2gTPP_Zcolz4.webp" alt="Diagram of the transformer deep learning architecture." width="561" height="271" loading="lazy" decoding="async" class="rounded-lg max-h-[35rem] w-max object-contain !px-0">

<section class="!max-w-full !px-0 py-6 w-full mx-0 flex justify-center" style="background-color: #e4e4e7;"> <div class="flex flex-col gap-4 items-center w-full max-w-[60rem] [&>h2]:mt-0 px-6"> <h2 id="abstract">Abstract</h2><p>Prompting Large Language Models (LLMs), or providing context on the expected model of operation, is an effective way to steer the outputs of such models to satisfy human desiderata after they have been trained. But in rapidly evolving domains, there is often need to fine-tune LLMs to improve either the kind of knowledge in their memory or their abilities to perform open ended reasoning in new domains. When human’s learn new concepts, we often do so by linking the new material that we are studying to concepts we have already learned before. To that end, we ask, “can prompting help us teach LLMs how to learn”. In this work, we study a novel generalization of instruction tuning, called contextual fine-tuning, to fine-tune LLMs. Our method leverages instructional prompts designed to mimic human cognitive strategies in learning and problem-solving to guide the learning process during training, aiming to improve the model’s interpretation and understanding of domain-specific knowledge. We empirically demonstrate that this simple yet effective modification improves the ability of LLMs to be fine-tuned rapidly on new datasets both within the medical and financial domains.</p> </div> </section>
<h2 id="overview">Overview</h2>
<p>Contextual Fine-Tuning (CFT) is a novel approach that blends in-context learning with gradient-based learning to improve how Large Language Models (LLMs) learn domain-specific knowledge. Unlike traditional continued pre-training or instruction fine-tuning, CFT uses contextual prompts to guide the model’s learning process.</p>
<h2 id="openmedtext">OpenMedText</h2>
<p>To test domain adaptation in a challenging setting, we curated a biomedical dataset of 121,489 biomedical journal articles (MDPI) and 29 open-source medical textbooks. We release <em>OpenMedText</em> as a resource for domain adaptation research.</p>
<h2 id="experiments">Experiments</h2>
<p>We evaluate on both <strong>medical</strong> and <strong>financial</strong> tasks to show the effectiveness of CFT.</p>
<p><strong>Medical tasks</strong> include subsets of MMLU including Anatomy, Clinical Knowledge, College Biology, College Medicine, Medical Genetics, Professional Medicine, and the MedQA professional exam dataset.</p>
<p><strong>Financial tasks</strong> include FiQA sentiment analysis, MultiFin headline classification, and Causal20.</p>
<h3 id="setup">Setup</h3>
<ul>
<li>We fine-tune Llama 2 models of size 7B and 13B.</li>
<li>We compare baseline Chat, Chat with continued pre-training (CPT), and Chat (CFT) for each domain’s training corpus.</li>
<li>We measure zero-shot performance on the downstream tasks.</li>
</ul>
<h3 id="results-medical-domain">Results (Medical Domain)</h3>
<h4 id="7b-model-results">7B Model Results</h4>




























































<div class="px-6 w-full"> <table class="w-full"> <thead><tr><th style="text-align:center">Model (7B)</th><th style="text-align:center">Anatomy</th><th style="text-align:center">Clinical Knowledge</th><th style="text-align:center">College Biology</th><th style="text-align:center">College Medicine</th><th style="text-align:center">Medical Genetics</th><th style="text-align:center">Professional Medicine</th><th style="text-align:center">MedQA</th><th style="text-align:center">Average</th></tr></thead><tbody><tr><td style="text-align:center">Chat</td><td style="text-align:center">44.07</td><td style="text-align:center">46.79</td><td style="text-align:center">48.61</td><td style="text-align:center">39.02</td><td style="text-align:center">49.00</td><td style="text-align:center"><strong>48.90</strong></td><td style="text-align:center">38.96</td><td style="text-align:center">45.05</td></tr><tr><td style="text-align:center">Chat (CPT)</td><td style="text-align:center">45.19</td><td style="text-align:center">47.17</td><td style="text-align:center">49.31</td><td style="text-align:center">43.93</td><td style="text-align:center">50.50</td><td style="text-align:center">46.32</td><td style="text-align:center">39.28</td><td style="text-align:center">45.96</td></tr><tr><td style="text-align:center">Chat (CFT)</td><td style="text-align:center"><strong>48.15</strong></td><td style="text-align:center"><strong>48.87</strong></td><td style="text-align:center"><strong>52.08</strong></td><td style="text-align:center"><strong>44.22</strong></td><td style="text-align:center"><strong>54.00</strong></td><td style="text-align:center">46.69</td><td style="text-align:center"><strong>40.65</strong></td><td style="text-align:center"><strong>47.81</strong></td></tr><tr><td style="text-align:center">AdaptLLM</td><td style="text-align:center">44.45</td><td style="text-align:center">47.36</td><td style="text-align:center">48.27</td><td style="text-align:center">39.60</td><td style="text-align:center">45.00</td><td style="text-align:center">38.61</td><td style="text-align:center">37.12</td><td style="text-align:center">42.92</td></tr></tbody> </table> </div>
<ul>
<li>CFT outperforms CPT by 1.85% and significantly outperforms AdaptLLM by 4.89% on average, demonstrating its superior effectiveness for domain adaptation in medical contexts.</li>
</ul>
<h4 id="13b-model-results">13B Model Results</h4>

















































<div class="px-6 w-full"> <table class="w-full"> <thead><tr><th style="text-align:center">Model (13B)</th><th style="text-align:center">Anatomy</th><th style="text-align:center">Clinical Knowledge</th><th style="text-align:center">College Biology</th><th style="text-align:center">College Medicine</th><th style="text-align:center">Medical Genetics</th><th style="text-align:center">Professional Medicine</th><th style="text-align:center">MedQA</th><th style="text-align:center">Average</th></tr></thead><tbody><tr><td style="text-align:center">Chat</td><td style="text-align:center">51.85</td><td style="text-align:center">56.60</td><td style="text-align:center">54.17</td><td style="text-align:center">46.82</td><td style="text-align:center"><strong>63.50</strong></td><td style="text-align:center">56.99</td><td style="text-align:center"><strong>45.33</strong></td><td style="text-align:center">53.61</td></tr><tr><td style="text-align:center">Chat (CPT)</td><td style="text-align:center">50.37</td><td style="text-align:center">60.00</td><td style="text-align:center">55.90</td><td style="text-align:center">50.58</td><td style="text-align:center">62.00</td><td style="text-align:center">57.35</td><td style="text-align:center">43.95</td><td style="text-align:center">54.31</td></tr><tr><td style="text-align:center">Chat (CFT)</td><td style="text-align:center"><strong>53.33</strong></td><td style="text-align:center"><strong>63.21</strong></td><td style="text-align:center"><strong>57.99</strong></td><td style="text-align:center"><strong>56.35</strong></td><td style="text-align:center">62.50</td><td style="text-align:center"><strong>57.72</strong></td><td style="text-align:center">44.85</td><td style="text-align:center"><strong>56.56</strong></td></tr></tbody> </table> </div>
<ul>
<li>The 13B model shows consistent improvements with CFT, achieving a 2.25% gain over CPT and 2.95% over the baseline Chat model, with strongest performance in Clinical Knowledge, College Biology, and College Medicine tasks.</li>
</ul>
<h3 id="results-financial-domain">Results (Financial Domain)</h3>
<h4 id="7b-model-results-1">7B Model Results</h4>

































<div class="px-6 w-full"> <table class="w-full"> <thead><tr><th style="text-align:center">Model (7B)</th><th style="text-align:center">FiQA (F1)</th><th style="text-align:center">Causal20 (F1)</th><th style="text-align:center">MultiFin (F1)</th><th style="text-align:center">Average</th></tr></thead><tbody><tr><td style="text-align:center">Chat</td><td style="text-align:center">56.40</td><td style="text-align:center"><strong>90.40</strong></td><td style="text-align:center">38.74</td><td style="text-align:center">61.48</td></tr><tr><td style="text-align:center">Chat (CPT)</td><td style="text-align:center">62.53</td><td style="text-align:center">90.16</td><td style="text-align:center">38.23</td><td style="text-align:center">63.64</td></tr><tr><td style="text-align:center">Chat (CFT)</td><td style="text-align:center"><strong>67.69</strong></td><td style="text-align:center">90.17</td><td style="text-align:center"><strong>46.01</strong></td><td style="text-align:center"><strong>67.96</strong></td></tr></tbody> </table> </div>
<ul>
<li>CFT achieves a 4.32% improvement over CPT, with particularly strong gains in financial sentiment analysis (FiQA) and headline classification (MultiFin).</li>
</ul>
<h4 id="13b-model-results-1">13B Model Results</h4>

































<div class="px-6 w-full"> <table class="w-full"> <thead><tr><th style="text-align:center">Model (13B)</th><th style="text-align:center">FiQA (F1)</th><th style="text-align:center">Causal20 (F1)</th><th style="text-align:center">MultiFin (F1)</th><th style="text-align:center">Average</th></tr></thead><tbody><tr><td style="text-align:center">Chat</td><td style="text-align:center">61.18</td><td style="text-align:center">84.77</td><td style="text-align:center">45.81</td><td style="text-align:center">63.92</td></tr><tr><td style="text-align:center">Chat (CPT)</td><td style="text-align:center">66.96</td><td style="text-align:center"><strong>90.06</strong></td><td style="text-align:center">45.33</td><td style="text-align:center">67.45</td></tr><tr><td style="text-align:center">Chat (CFT)</td><td style="text-align:center"><strong>70.55</strong></td><td style="text-align:center">89.87</td><td style="text-align:center"><strong>50.94</strong></td><td style="text-align:center"><strong>70.45</strong></td></tr></tbody> </table> </div>
<ul>
<li>In financial benchmarks, the 13B model with CFT achieves a 3.0% improvement over CPT and 6.53% over the baseline Chat model, demonstrating CFT’s effectiveness scales with model size.</li>
</ul>
<h2 id="bibtex-citation">BibTeX citation</h2>
<div class="px-6 w-full"> <pre class="relative flex flex-col whitespace-pre overflow-auto bg-zinc-300 p-4 rounded-lg w-full">    <code><span class="line"><span style="color:#D73A49">@inproceedings</span><span style="color:#24292E">{</span></span>
<span class="line"><span style="color:#24292E">choi2025teaching,</span></span>
<span class="line"><span style="color:#005CC5">title</span><span style="color:#24292E">=</span><span style="color:#032F62">{</span><span style="color:#24292E">Teaching {LLM}s How To Learn with Contextual Fine-Tuning</span><span style="color:#032F62">}</span><span style="color:#24292E">,</span></span>
<span class="line"><span style="color:#005CC5">author</span><span style="color:#24292E">=</span><span style="color:#032F62">{</span><span style="color:#24292E">Younwoo Choi and Muhammad Adil Asif and Ziwen Han and John Willes and Rahul G. Krishnan</span><span style="color:#032F62">}</span><span style="color:#24292E">,</span></span>
<span class="line"><span style="color:#005CC5">booktitle</span><span style="color:#24292E">=</span><span style="color:#032F62">{</span><span style="color:#24292E">The Thirteenth International Conference on Learning Representations</span><span style="color:#032F62">}</span><span style="color:#24292E">,</span></span>
<span class="line"><span style="color:#005CC5">year</span><span style="color:#24292E">=</span><span style="color:#032F62">{</span><span style="color:#24292E">2025</span><span style="color:#032F62">}</span><span style="color:#24292E">,</span></span>
<span class="line"><span style="color:#005CC5">url</span><span style="color:#24292E">=</span><span style="color:#032F62">{</span><span style="color:#24292E">https://arxiv.org/abs/2503.09032</span><span style="color:#032F62">}</span></span>
<span class="line"><span style="color:#24292E">}</span></span></code>
  </pre> </div> <footer class="m-auto"> <p class="text-zinc-500 text-sm">
This page was built using <a href="https://github.com/RomanHauksson/academic-project-astro-template">Roman Hauksson's academic project page template</a>, which was adapted from <a href="https://github.com/eliahuhorwitz/Academic-project-page-template">Eliahu Horwitz's template</a>, which was adapted from <a href="https://nerfies.github.io/">Keunhong Park's project page for <i>Nerfies</i></a>. It is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
</p> </footer> </body></html>